{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"An event-driven approach to serverless seismic imaging in the cloud This documentation describes how to set up the workflow described in the paper \"An event-driven approach to serverless seismic imaging in the cloud\". The documentation also provides instructions how to reproduce the numerical examples and performance tests. Setting up the following seismic imaging workflow requires an Amazon Web services (AWS) account. To reproduce the workflow, follow the instructions of this documentation step-by-step. Please do not leave out any steps, as this will result in a malfunctioning workflow. Disclaimer : Some of the services and instance types used for the examples are not available as part of the AWS free tier and will invoke charges Necessary AWS credentials The steps for reproducing the workflow require access to the IAM user name and password (to log into the AWS console), as well as to the AWS Access Key ID and the AWS Secret Access key . Install and configure AWS Command Line Interface The first required step is the installation of the AWS command line interface (CLI). With pip3 , the CLI can be installed from a Linux/Unix terminal by executing the following command: pip3 install awscli --upgrade --user Once installed, the CLI must be configured with the AWS user credentials. Run aws configure from the command line and enter your AWS Access Key ID, the AWS Secret Access Key and a region name (e.g. us-east-1 ). Clone github repository Next, clone the Github repository that contains all setup scripts and the numerical examples. Here, we add the repository to the home directory ( ~/ ): git clone https://github.com/philippwitte/cloud-imaging ~/.","title":"Home"},{"location":"#an-event-driven-approach-to-serverless-seismic-imaging-in-the-cloud","text":"This documentation describes how to set up the workflow described in the paper \"An event-driven approach to serverless seismic imaging in the cloud\". The documentation also provides instructions how to reproduce the numerical examples and performance tests. Setting up the following seismic imaging workflow requires an Amazon Web services (AWS) account. To reproduce the workflow, follow the instructions of this documentation step-by-step. Please do not leave out any steps, as this will result in a malfunctioning workflow. Disclaimer : Some of the services and instance types used for the examples are not available as part of the AWS free tier and will invoke charges","title":"An event-driven approach to serverless seismic imaging in the cloud"},{"location":"#necessary-aws-credentials","text":"The steps for reproducing the workflow require access to the IAM user name and password (to log into the AWS console), as well as to the AWS Access Key ID and the AWS Secret Access key .","title":"Necessary AWS credentials"},{"location":"#install-and-configure-aws-command-line-interface","text":"The first required step is the installation of the AWS command line interface (CLI). With pip3 , the CLI can be installed from a Linux/Unix terminal by executing the following command: pip3 install awscli --upgrade --user Once installed, the CLI must be configured with the AWS user credentials. Run aws configure from the command line and enter your AWS Access Key ID, the AWS Secret Access Key and a region name (e.g. us-east-1 ).","title":"Install and configure AWS Command Line Interface"},{"location":"#clone-github-repository","text":"Next, clone the Github repository that contains all setup scripts and the numerical examples. Here, we add the repository to the home directory ( ~/ ): git clone https://github.com/philippwitte/cloud-imaging ~/.","title":"Clone github repository"},{"location":"about/","text":"Authors This documentation was created by Philipp A. Witte ( pwitte3@gatech.edu ) from the Georgia Institute of Technology. People involved in the development of the workflow include: Philipp A. Witte (Georgia Institute of Technology) Mathias Louboutin (Georgia Institute of Technology) Henryk Modzelewski (The Univeristy of British Columbia) Charles Jones (Osokey Ltd.) James Selvage (Osokey Ltd.) Felix J. Herrmann (Georgia Institute of Technology) The research of this project was carried out at the Seismic Laboratory for Imaging and Modeling (SLIM) at the Georgia Institute of Technology, Atlanta, USA.","title":"About"},{"location":"about/#authors","text":"This documentation was created by Philipp A. Witte ( pwitte3@gatech.edu ) from the Georgia Institute of Technology. People involved in the development of the workflow include: Philipp A. Witte (Georgia Institute of Technology) Mathias Louboutin (Georgia Institute of Technology) Henryk Modzelewski (The Univeristy of British Columbia) Charles Jones (Osokey Ltd.) James Selvage (Osokey Ltd.) Felix J. Herrmann (Georgia Institute of Technology) The research of this project was carried out at the Seismic Laboratory for Imaging and Modeling (SLIM) at the Georgia Institute of Technology, Atlanta, USA.","title":"Authors"},{"location":"batch/","text":"Batch environment To run AWS Batch jobs, we need to first set up Compute environments and Job queues . The compute environments essentially specify the virtual cluster that AWS Batch has access to. This involves specifying which type of instances are allowed, as well as their size (i.e. number of CPUs and memory). For multi-node AWS Batch jobs, we also have to set up a shared file system and a customized Amazon Machine Image (AMI). Elastic file system Setting up an elastic file system is only necessary for multi-node batch jobs , i.e. for AWS Batch jobs that run each job on multiple EC2 instances. For single-node batch jobs , skip this part and proceed to the next section. For multi-node AWS Batch jobs, we need to set up a shared file system called elastic file system (EFS). Furthermore, we need to set up a customized Amazon Machine Image (AMI) and mount the shared file system. Detailed instructions for these steps are provided in the AWS documentation: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_efs.html . Here, we provide a summary of the necessary steps: 1) Create an elastic file system by logging into the AWS console in the web browser and go to Services -> EFS -> Create file system . By default, AWS will fill in all available subnets and include the default security group. For each zone, also add the SSH-security group. Proceed to step 2 and 3 and then select Create File System . 2) Next, we have to modify the AMI that is used by AWS Batch and mount the file system. For this, we launch an EC2 instances with the ECS-optimized AMI, mount the EFS and create a custom AMI, which will then be used in the compute environment. Choose the Amazon Linux 2 AMI for your region from the following list: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html . For example, for the us-east-1 region, this is AMI ID ami-0fac5486e4cff37f4 . Click on Launch Instance to request an EC2 instance with the corresponding AMI. Using the t2.micro instance type is sufficient for this task. Next, connect to your instance via ssh : ssh -Y -i ~/.ssh/user_key_pair -o StrictHostKeyChecking=no -l ec2-user public_DNS_of_instance Once you are logged into the instance, following the subsequent steps: Create mount point: sudo mkdir /efs Install the amazon-efs-utils client software: sudo yum install -y amazon-efs-utils Make a backup of the /etc/fstab file: sudo cp /etc/fstab /etc/fstab.bak Open the original file with sudo vi /etc/fstab and add the following line to it. Replace efs_dns_name with the DNS name of your elastic file system (find the DNS name in the AWS console -> Services -> EFS -> name-of-your-file-system -> DNS name ): efs_dns_name:/ /efs nfs4 nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0 Reload file system: sudo mount -a Validate that file system is mounted correctly: mount | grep efs Log out from the instance and create a new AMI from the running EC2 instance. Go the list of running EC2 instances in the AWS console and select your running instance -> Actions -> Image -> Create Image . Choose an image name and then hit Create Image . AMIs without hyper-threading By default, AWS Batch uses hyperthreading (HT) on the underlying EC2 instances. For our workflow, we need to disable HT and limit the number of cores to half the number of virtual CPU cores on the corresponding EC2 instance. For example, the r5.24xlarge instance has 96 virtual CPUs and therefore 48 physical cores. To disable HT for this instance, we need to set the maximum number of allowed CPUs to 48. To disable HT, we modify the AMI that is used by AWS Batch. For this, we launch an EC2 instances with the ECS-optimized AMI, specify the maximum number of allowed CPUs and create a custom AMI. This AMI will then be used in the compute environment. If you already created an AMI in the previous section with an elastic file system, start a new EC2 instance using this AMI and connect to your instance. If you have not created an AMI yet, choose the Amazon Linux 2 AMI for your region from the following list: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html . For example, for the us-east-1 region, this is AMI ID ami-0fac5486e4cff37f4 . Click on Launch Instance to request an EC2 instance with the corresponding AMI. Using the t2.micro instance type is sufficient for this task. Next, connect to your instance via ssh : ssh -Y -i ~/.ssh/user_key_pair -o StrictHostKeyChecking=no -l ec2-user public_DNS_of_instance Open the grub config file with sudo vi /etc/default/grub and add nr_cpus=48 to the line starting with GRUB_CMDLINE_LINUX (or however many cores are required). Apply the changes by running: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Log out from the instance and create a new AMI from the running EC2 instance. Go the list of running EC2 instances in the AWS console and select your running instance -> Actions -> Image -> Create Image . Choose an image name that indicates the maximum number of cores and then hit Create Image . Follow the same steps to create customized AMIs for other instance types with a differet number of CPU cores. E.g. for the r5.12xlarge instance, set nr_cpus=24 , as this instance type has 48 vCPUs with 24 physicsal cores. For the c5n.18xlarge instance (72 vCPUs), set nr_cpus=36 and so on. Important : To reproduce the numerical examples, create a total of 4 AMIs: one with a maximum of 8 cores, one with 24 cores, one with 18 cores and one with 48 cores. For multi-node batch jobs, always start the above process from the AMI that has the mounted elastic file system. Otherwise, start from the Amazon Linux 2 AMI. Create environments The performance tests in the manuscript are carried out on several different compute environments. Most examples are run in the M4_SPOT_MAXCPU_8 environment, using m4.4xlarge Spot instances with 8 physical cores per instance. The compute environment can be set up from the command line, with all parameters being specified in the ~/cloud-imaging/batch/create_environment_m4_spot.json file. Open the file and fill in all missing entries . These are: spotIamFleetRole : Go to the AWS console -> Services -> IAM -> Roles and find the AmazonEC2SpotFleetRole . Copy the role ARN and paste it. subnets : Find your subnets in the AWS console at Services -> VPC -> Subnets . Copy the Subnet ID of each subnet into the parameter file (separated by commas). securityGroupIds : Find the security groups in the console at Services -> EC2 -> Security groups . Copy-paste the Group ID of the default security group. To enable ssh access to instances of AWS Batch jobs, optionally create and add an SSH security group to this list. ec2KeyPair : To connect to running instances via ssh, add the name of your AWS ssh key pair. imageId : Go to the console -> Services -> EC2 -> AMIs and find the AMI that was created in the previous step. For the M4_SPOT_MAXCPU_8 compute environment, find the AMI with 8 cores and copy-paste the AMI-ID into the parameter file. instanceRole : Go to the AWS console -> Services -> IAM -> Roles . Find the SLIM-Extras_ECS_for_EC2 role and add its ARN to the parameter file. serviceRole : Go to the AWS console -> Services -> IAM -> Roles . Find the SLIM-AWSBatchServiceRole role and add its ARN. Do not modify the parameters that are already filled in. Save the updated file and then run the following command within the ~/cloud-imaging directory: # Create environment aws batch create-compute-environment --cli-input-json file://batch/create_environment_m4_spot.json You can go to the AWS Console in the web browser and move to Services -> AWS Batch -> Compute environments to verify that the environment has been created successfully. To reproduce the numerical examples and performance tests, fill in the missing entries of the remaining parameter files: create_environment_r5_spot_24.json and create_environment_r5_spot_48.json . For the former, select the AMI with 24 cores and for the latter the AMI with 48 cores. Then re-run the above command for these parameter files: # Create environment w/ r5.24xlarge instances aws batch create-compute-environment --cli-input-json file://batch/create_environment_r5_spot_24.json # Create bybrid OMP-MPI environment w/ r5.24xlarge instances aws batch create-compute-environment --cli-input-json file://batch/create_environment_r5_spot_48.json Create queues For each compute environment, we need to create an AWS Batch Job queue, to which our workflow will submit its work loads. The queue parameter files do not need to be modified, so simply run the following commands from the terminal within the ~/cloud-imaging directory: # Job queue M4 environment aws batch create-job-queue --cli-input-json file://batch/create_queue_m4_spot.json # Job queue R5 environment (24 cores) aws batch create-job-queue --cli-input-json file://batch/create_queue_r5_spot_24.json # Job queue R5 environment (48 cores) aws batch create-job-queue --cli-input-json file://batch/create_queue_r5_spot_48.json Multi-node environment and queues For multi-node AWS Batch jobs we have to set up compute environments similar to the ones as specified above, with the major difference that multi-node batch jobs do not support spot instances. We therefore set up on-demand compute environments for the r5.24xlarge and the c5n.18xlarge instance type. The environment parameters are specified in the files create_environment_r5_multinode_24.json and create_environment_c5n_multinode_18.json . Fill in the missing parameters like in the above example. For the AMI field, enter the AMI ID of the AMI with the elastic file system. Use the AMI with a maximum of 24 CPUs for the r5 environment and the AMI with 18 cores for the c5n environment. The parameter files also specify a placement group called MPIgroup . The placement group ensures that EC2 instances of the MPI clusters are in close physical vicinity to each other. Create the MPIgroup placement group from the AWS console Services -> EC2 -> Placement Groups -> Create Placement Group . Enter the name MPIgroup and select Cluster in the Strategy field. Then click the create button. After creating the placement group, generate the compute environments as in the above example: # Create on-demand environment w/ r5.24xlarge instances aws batch create-compute-environment --cli-input-json file://batch/create_environment_r5_multinode_24.json # Create on-demand environment w/ c5n.18xlarge instances aws batch create-compute-environment --cli-input-json file://batch/create_environment_c5n_multinode_18.json Finally, create the corresponding batch queues by running the following commands: # Job queue r5 on-demand aws batch create-job-queue --cli-input-json file://batch/create_queue_r5_multinode_24.json # Job queue c5n on-demand aws batch create-job-queue --cli-input-json file://batch/create_queue_c5n_multinode_18.json VPC endpoints For multi-node batch jobs, follow these steps to create endpoints for S3 and SQS in your virtual privat cloud: Log into the AWS console in the browser and go to: Services -> VPC -> Endpoints . Create an S3 endpoint: click Create Endpoint and select the S3 service name from the list, e.g. com.amazonaws.us-east-1.s3 . Next, select the only available route table in the section Configure route tables . Finalize the endpoint by clicking the Create Endpoint button. Create an SQS endpoint: click Create Endpoint and select the SQS service name from the list, e.g. com.amazonaws.us-east-1.sqs . Ensure that all availability zones are selected. Under Security group , select the default security group, as well as the SSH group. Finalize the endpoint by clicking the Create Endpoint button.","title":"Set up batch environment"},{"location":"batch/#batch-environment","text":"To run AWS Batch jobs, we need to first set up Compute environments and Job queues . The compute environments essentially specify the virtual cluster that AWS Batch has access to. This involves specifying which type of instances are allowed, as well as their size (i.e. number of CPUs and memory). For multi-node AWS Batch jobs, we also have to set up a shared file system and a customized Amazon Machine Image (AMI).","title":"Batch environment"},{"location":"batch/#elastic-file-system","text":"Setting up an elastic file system is only necessary for multi-node batch jobs , i.e. for AWS Batch jobs that run each job on multiple EC2 instances. For single-node batch jobs , skip this part and proceed to the next section. For multi-node AWS Batch jobs, we need to set up a shared file system called elastic file system (EFS). Furthermore, we need to set up a customized Amazon Machine Image (AMI) and mount the shared file system. Detailed instructions for these steps are provided in the AWS documentation: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_efs.html . Here, we provide a summary of the necessary steps: 1) Create an elastic file system by logging into the AWS console in the web browser and go to Services -> EFS -> Create file system . By default, AWS will fill in all available subnets and include the default security group. For each zone, also add the SSH-security group. Proceed to step 2 and 3 and then select Create File System . 2) Next, we have to modify the AMI that is used by AWS Batch and mount the file system. For this, we launch an EC2 instances with the ECS-optimized AMI, mount the EFS and create a custom AMI, which will then be used in the compute environment. Choose the Amazon Linux 2 AMI for your region from the following list: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html . For example, for the us-east-1 region, this is AMI ID ami-0fac5486e4cff37f4 . Click on Launch Instance to request an EC2 instance with the corresponding AMI. Using the t2.micro instance type is sufficient for this task. Next, connect to your instance via ssh : ssh -Y -i ~/.ssh/user_key_pair -o StrictHostKeyChecking=no -l ec2-user public_DNS_of_instance Once you are logged into the instance, following the subsequent steps: Create mount point: sudo mkdir /efs Install the amazon-efs-utils client software: sudo yum install -y amazon-efs-utils Make a backup of the /etc/fstab file: sudo cp /etc/fstab /etc/fstab.bak Open the original file with sudo vi /etc/fstab and add the following line to it. Replace efs_dns_name with the DNS name of your elastic file system (find the DNS name in the AWS console -> Services -> EFS -> name-of-your-file-system -> DNS name ): efs_dns_name:/ /efs nfs4 nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0 Reload file system: sudo mount -a Validate that file system is mounted correctly: mount | grep efs Log out from the instance and create a new AMI from the running EC2 instance. Go the list of running EC2 instances in the AWS console and select your running instance -> Actions -> Image -> Create Image . Choose an image name and then hit Create Image .","title":"Elastic file system"},{"location":"batch/#amis-without-hyper-threading","text":"By default, AWS Batch uses hyperthreading (HT) on the underlying EC2 instances. For our workflow, we need to disable HT and limit the number of cores to half the number of virtual CPU cores on the corresponding EC2 instance. For example, the r5.24xlarge instance has 96 virtual CPUs and therefore 48 physical cores. To disable HT for this instance, we need to set the maximum number of allowed CPUs to 48. To disable HT, we modify the AMI that is used by AWS Batch. For this, we launch an EC2 instances with the ECS-optimized AMI, specify the maximum number of allowed CPUs and create a custom AMI. This AMI will then be used in the compute environment. If you already created an AMI in the previous section with an elastic file system, start a new EC2 instance using this AMI and connect to your instance. If you have not created an AMI yet, choose the Amazon Linux 2 AMI for your region from the following list: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html . For example, for the us-east-1 region, this is AMI ID ami-0fac5486e4cff37f4 . Click on Launch Instance to request an EC2 instance with the corresponding AMI. Using the t2.micro instance type is sufficient for this task. Next, connect to your instance via ssh : ssh -Y -i ~/.ssh/user_key_pair -o StrictHostKeyChecking=no -l ec2-user public_DNS_of_instance Open the grub config file with sudo vi /etc/default/grub and add nr_cpus=48 to the line starting with GRUB_CMDLINE_LINUX (or however many cores are required). Apply the changes by running: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Log out from the instance and create a new AMI from the running EC2 instance. Go the list of running EC2 instances in the AWS console and select your running instance -> Actions -> Image -> Create Image . Choose an image name that indicates the maximum number of cores and then hit Create Image . Follow the same steps to create customized AMIs for other instance types with a differet number of CPU cores. E.g. for the r5.12xlarge instance, set nr_cpus=24 , as this instance type has 48 vCPUs with 24 physicsal cores. For the c5n.18xlarge instance (72 vCPUs), set nr_cpus=36 and so on. Important : To reproduce the numerical examples, create a total of 4 AMIs: one with a maximum of 8 cores, one with 24 cores, one with 18 cores and one with 48 cores. For multi-node batch jobs, always start the above process from the AMI that has the mounted elastic file system. Otherwise, start from the Amazon Linux 2 AMI.","title":"AMIs without hyper-threading"},{"location":"batch/#create-environments","text":"The performance tests in the manuscript are carried out on several different compute environments. Most examples are run in the M4_SPOT_MAXCPU_8 environment, using m4.4xlarge Spot instances with 8 physical cores per instance. The compute environment can be set up from the command line, with all parameters being specified in the ~/cloud-imaging/batch/create_environment_m4_spot.json file. Open the file and fill in all missing entries . These are: spotIamFleetRole : Go to the AWS console -> Services -> IAM -> Roles and find the AmazonEC2SpotFleetRole . Copy the role ARN and paste it. subnets : Find your subnets in the AWS console at Services -> VPC -> Subnets . Copy the Subnet ID of each subnet into the parameter file (separated by commas). securityGroupIds : Find the security groups in the console at Services -> EC2 -> Security groups . Copy-paste the Group ID of the default security group. To enable ssh access to instances of AWS Batch jobs, optionally create and add an SSH security group to this list. ec2KeyPair : To connect to running instances via ssh, add the name of your AWS ssh key pair. imageId : Go to the console -> Services -> EC2 -> AMIs and find the AMI that was created in the previous step. For the M4_SPOT_MAXCPU_8 compute environment, find the AMI with 8 cores and copy-paste the AMI-ID into the parameter file. instanceRole : Go to the AWS console -> Services -> IAM -> Roles . Find the SLIM-Extras_ECS_for_EC2 role and add its ARN to the parameter file. serviceRole : Go to the AWS console -> Services -> IAM -> Roles . Find the SLIM-AWSBatchServiceRole role and add its ARN. Do not modify the parameters that are already filled in. Save the updated file and then run the following command within the ~/cloud-imaging directory: # Create environment aws batch create-compute-environment --cli-input-json file://batch/create_environment_m4_spot.json You can go to the AWS Console in the web browser and move to Services -> AWS Batch -> Compute environments to verify that the environment has been created successfully. To reproduce the numerical examples and performance tests, fill in the missing entries of the remaining parameter files: create_environment_r5_spot_24.json and create_environment_r5_spot_48.json . For the former, select the AMI with 24 cores and for the latter the AMI with 48 cores. Then re-run the above command for these parameter files: # Create environment w/ r5.24xlarge instances aws batch create-compute-environment --cli-input-json file://batch/create_environment_r5_spot_24.json # Create bybrid OMP-MPI environment w/ r5.24xlarge instances aws batch create-compute-environment --cli-input-json file://batch/create_environment_r5_spot_48.json","title":"Create environments"},{"location":"batch/#create-queues","text":"For each compute environment, we need to create an AWS Batch Job queue, to which our workflow will submit its work loads. The queue parameter files do not need to be modified, so simply run the following commands from the terminal within the ~/cloud-imaging directory: # Job queue M4 environment aws batch create-job-queue --cli-input-json file://batch/create_queue_m4_spot.json # Job queue R5 environment (24 cores) aws batch create-job-queue --cli-input-json file://batch/create_queue_r5_spot_24.json # Job queue R5 environment (48 cores) aws batch create-job-queue --cli-input-json file://batch/create_queue_r5_spot_48.json","title":"Create queues"},{"location":"batch/#multi-node-environment-and-queues","text":"For multi-node AWS Batch jobs we have to set up compute environments similar to the ones as specified above, with the major difference that multi-node batch jobs do not support spot instances. We therefore set up on-demand compute environments for the r5.24xlarge and the c5n.18xlarge instance type. The environment parameters are specified in the files create_environment_r5_multinode_24.json and create_environment_c5n_multinode_18.json . Fill in the missing parameters like in the above example. For the AMI field, enter the AMI ID of the AMI with the elastic file system. Use the AMI with a maximum of 24 CPUs for the r5 environment and the AMI with 18 cores for the c5n environment. The parameter files also specify a placement group called MPIgroup . The placement group ensures that EC2 instances of the MPI clusters are in close physical vicinity to each other. Create the MPIgroup placement group from the AWS console Services -> EC2 -> Placement Groups -> Create Placement Group . Enter the name MPIgroup and select Cluster in the Strategy field. Then click the create button. After creating the placement group, generate the compute environments as in the above example: # Create on-demand environment w/ r5.24xlarge instances aws batch create-compute-environment --cli-input-json file://batch/create_environment_r5_multinode_24.json # Create on-demand environment w/ c5n.18xlarge instances aws batch create-compute-environment --cli-input-json file://batch/create_environment_c5n_multinode_18.json Finally, create the corresponding batch queues by running the following commands: # Job queue r5 on-demand aws batch create-job-queue --cli-input-json file://batch/create_queue_r5_multinode_24.json # Job queue c5n on-demand aws batch create-job-queue --cli-input-json file://batch/create_queue_c5n_multinode_18.json","title":"Multi-node environment and queues"},{"location":"batch/#vpc-endpoints","text":"For multi-node batch jobs, follow these steps to create endpoints for S3 and SQS in your virtual privat cloud: Log into the AWS console in the browser and go to: Services -> VPC -> Endpoints . Create an S3 endpoint: click Create Endpoint and select the S3 service name from the list, e.g. com.amazonaws.us-east-1.s3 . Next, select the only available route table in the section Configure route tables . Finalize the endpoint by clicking the Create Endpoint button. Create an SQS endpoint: click Create Endpoint and select the SQS service name from the list, e.g. com.amazonaws.us-east-1.sqs . Ensure that all availability zones are selected. Under Security group , select the default security group, as well as the SSH group. Finalize the endpoint by clicking the Create Endpoint button.","title":"VPC endpoints"},{"location":"docker/","text":"Docker The required Docker images for the workflow are publicly available on Docker hub. The Docker image used by AWS Batch is specified in the job parameter files of the numerical examples (e.g. ~/cloud-imaging/numerical_examples/imaging_example_sgd/parameters.json ). In the current set up, all single-node batch jobs use the pre-existing Docker image aws_seismic_imaging:v1.0 . To (optionally) obtain a local copy of the image, run: docker pull philippwitte/aws_seismic_imaging:v1.0 All multi-node batch jobs use the per-exisiting Docker image aws_seismic_imaging_mpi:v1.0 . Obtain an (optional) local copy with: docker pull philippwitte/aws_seismic_imaging_mpi:v1.0 To reproduce the numerical examples, no actions regarding Docker are necessary. Single-node batch jobs To update or modify the Docker container for single node batch jobs, first pull the current version of the image with the above command. Update the Dockerfile in ~/cloud-imaging/docker/single_node_batch as desired and then rebuild the Docker image with a new tag (e.g. v1.1 ): cd ~/cloud-imaging/docker/single_node_batch docker build -t aws_seismic_imaging:tag . As users cannot store their modified images on the original philippwitte Docker hub, you will upload the new image to your AWS account. First, you need to obtain login credentials by AWS. Copy-paste the output of the following command back into the terminal: # Get ECR login credentials and use the token that is printed in the terminal aws ecr get-login --no-include-email If the log in was successful, you will see the message \"Login Succeeded\" in your terminal. Next, create a repository on AWS called aws_seismic_imaging : # Create repository (only first time) aws ecr create-repository --repository-name aws_seismic_imaging Now, tag the new image using the URI of the repository that you just created. To find the URI of your repository, go to the AWS console -> Services -> ECR -> aws_seismic_imaging . Tag your image by running: # tag image docker tag aws_seismic_imaging:tag URI:tag Finally, upload your Docker image to your AWS container registry: docker push URI:tag Note the full name of your new image ( URI:tag ) and update the job parameter files correspondingly. Multi-node batch jobs Updating or modifying the Docker image for multi-node AWS Batch jobs follows the same steps as the previous instructions. Namely, modify the Dockerfile in ~/cloud-imaging/docker/multi_node_batch and rebuild the image using a new tag (e.g. v1.1 ) cd ~/cloud-imaging/docker/multi_node_batch docker build -t aws_seismic_imaging_mpi:tag . Tag the image, obtain the AWS ECR log-in credentials and push it to the ECR by following the steps from above. (Replace all instances of aws_seismic_imaging in the instructions with aws_seismic_imaging_mpi .)","title":"Create docker container"},{"location":"docker/#docker","text":"The required Docker images for the workflow are publicly available on Docker hub. The Docker image used by AWS Batch is specified in the job parameter files of the numerical examples (e.g. ~/cloud-imaging/numerical_examples/imaging_example_sgd/parameters.json ). In the current set up, all single-node batch jobs use the pre-existing Docker image aws_seismic_imaging:v1.0 . To (optionally) obtain a local copy of the image, run: docker pull philippwitte/aws_seismic_imaging:v1.0 All multi-node batch jobs use the per-exisiting Docker image aws_seismic_imaging_mpi:v1.0 . Obtain an (optional) local copy with: docker pull philippwitte/aws_seismic_imaging_mpi:v1.0 To reproduce the numerical examples, no actions regarding Docker are necessary.","title":"Docker"},{"location":"docker/#single-node-batch-jobs","text":"To update or modify the Docker container for single node batch jobs, first pull the current version of the image with the above command. Update the Dockerfile in ~/cloud-imaging/docker/single_node_batch as desired and then rebuild the Docker image with a new tag (e.g. v1.1 ): cd ~/cloud-imaging/docker/single_node_batch docker build -t aws_seismic_imaging:tag . As users cannot store their modified images on the original philippwitte Docker hub, you will upload the new image to your AWS account. First, you need to obtain login credentials by AWS. Copy-paste the output of the following command back into the terminal: # Get ECR login credentials and use the token that is printed in the terminal aws ecr get-login --no-include-email If the log in was successful, you will see the message \"Login Succeeded\" in your terminal. Next, create a repository on AWS called aws_seismic_imaging : # Create repository (only first time) aws ecr create-repository --repository-name aws_seismic_imaging Now, tag the new image using the URI of the repository that you just created. To find the URI of your repository, go to the AWS console -> Services -> ECR -> aws_seismic_imaging . Tag your image by running: # tag image docker tag aws_seismic_imaging:tag URI:tag Finally, upload your Docker image to your AWS container registry: docker push URI:tag Note the full name of your new image ( URI:tag ) and update the job parameter files correspondingly.","title":"Single-node batch jobs"},{"location":"docker/#multi-node-batch-jobs","text":"Updating or modifying the Docker image for multi-node AWS Batch jobs follows the same steps as the previous instructions. Namely, modify the Dockerfile in ~/cloud-imaging/docker/multi_node_batch and rebuild the image using a new tag (e.g. v1.1 ) cd ~/cloud-imaging/docker/multi_node_batch docker build -t aws_seismic_imaging_mpi:tag . Tag the image, obtain the AWS ECR log-in credentials and push it to the ECR by following the steps from above. (Replace all instances of aws_seismic_imaging in the instructions with aws_seismic_imaging_mpi .)","title":"Multi-node batch jobs"},{"location":"lambda/","text":"Lambda functions All AWS Lambda functions used in the workflow are defined in the ~/cloud-imaging/lambda/ directory and each sub-directory contains the source code of the respective Lambda function. Some functions require the installation of additional python packages such as numpy . Follow the instructions to create deployment packages for the Lambda functions and to upload them to AWS. To create and upload the Lambda function, we need the Lambda service role that we created earlier. Log into the AWS console and go to Services -> IAM -> Roles -> SLIM-Extras_for_Lambda to find the ARN of this role. CreateQueues From within the ~/cloud-imaging/lambda/CreateQueues directory, run the following command to create a zip archive: cd ~/cloud-imaging/lambda/CreateQueues zip -r9 ../CreateQueue.zip . When uploading the function for the first time, run the following command to create a new Lambda function. Replace copy_paste_role_arn_here with the ARN of the SLIM-Extras_for_Lambda role (see above instructions of how to obtain the ARN): # Upload function aws lambda create-function --function-name CreateQueues \\ --zip-file fileb://../CreateQueues.zip \\ --runtime python3.6 \\ --timeout 300 \\ --role copy_paste_role_arn_here \\ --handler lambda_function.lambda_handler \\ --memory-size 128 If you make changes to the Lambda function and need to update an existing function, first create a new zip file with the above command and then run the following command: # Upload function aws lambda update-function-code --function-name IteratorStochastic \\ --zip-file fileb://../IteratorStochastic.zip Iterator Create the zip archive: cd ~/cloud-imaging/lambda/Iterator zip -r9 ../Iterator.zip . Create and upload the Iterator Lambda function: # Upload function aws lambda create-function --function-name Iterator \\ --zip-file fileb://../Iterator.zip \\ --runtime python3.6 \\ --timeout 10 \\ --role copy_past_role_arn_here \\ --handler lambda_function.lambda_handler \\ --memory-size 128 Update an existing function with a new archive: # Upload function aws lambda update-function-code --function-name Iterator \\ --zip-file fileb://../Iterator.zip ComputeGradients First, install numpy and boto3 inside the ComputeGradients directory and create a zip archive. (Lambda functions have boto3 pre-installed, but the default version is a deprecated version that misses some functionality, so we need to install the current release manually.) cd ~/cloud-imaging/lambda/ComputeGradients pip install --target . numpy boto3 zip -r9 ../ComputeGradients.zip . Then create and upload the ComputeGradients Lambda function: # Upload function aws lambda create-function --function-name ComputeGradients \\ --zip-file fileb://../ComputeGradients.zip \\ --runtime python3.6 \\ --timeout 900 \\ --role copy_past_role_arn_here \\ --handler lambda_function.gradient_handler \\ --memory-size 128 To update an existing function with a new archive, run: # Upload function aws lambda update-function-code --function-name ComputeGradients \\ --zip-file fileb://../ComputeGradients.zip For multi-node AWS Batch jobs, repeat the above steps for the ComputeGradients_MultiNode Lambda function. First create a zip archive called ComputeGradients_MultiNode.zip and then run: # Upload multi-node function aws lambda create-function --function-name ComputeGradients_MultiNode \\ --zip-file fileb://../ComputeGradients_MultiNode.zip \\ --runtime python3.6 \\ --timeout 900 \\ --role copy_past_role_arn_here \\ --handler lambda_function.gradient_handler \\ --memory-size 128 CheckS3ForVariable Install python packages and create archive: cd ~/cloud-imaging/lambda/CheckS3ForVariable pip install --target . boto3 zip -r9 ../CheckS3ForVariable.zip . Create and upload Lambda function: # Upload function aws lambda create-function --function-name CheckS3ForVariable \\ --zip-file fileb://../CheckS3ForVariable.zip \\ --runtime python3.6 \\ --timeout 10 \\ --role copy_past_role_arn_here \\ --handler lambda_function.lambda_handler \\ --memory-size 128 Update the function: # Upload function aws lambda update-function-code --function-name CheckS3ForVariable \\ --zip-file fileb://../CheckS3ForVariable.zip For multi-node AWS Batch jobs, repeat the above steps for the CheckS3ForVariable_MultiNode Lambda function. Create a new zip file called CheckS3ForVariable_MultiNode.zip and then run: # Upload function aws lambda create-function --function-name CheckS3ForVariable_MultiNode \\ --zip-file fileb://../CheckS3ForVariable_MultiNode.zip \\ --runtime python3.6 \\ --timeout 10 \\ --role copy_past_role_arn_here \\ --handler lambda_function.lambda_handler \\ --memory-size 128 ReduceGradients Install python packages and create archive: cd ~/cloud-imaging/lambda/ReduceGradients pip install --target . numpy boto3 zip -r9 ../ReduceGradients.zip . Create and upload Lambda function: # Upload function aws lambda create-function --function-name ReduceGradients \\ --zip-file fileb://../ReduceGradients.zip \\ --runtime python3.6 \\ --timeout 60 \\ --role copy_past_role_arn_here \\ --handler lambda_function.lambda_handler \\ --memory-size 512 Update the function: # Upload function aws lambda update-function-code --function-name CheckS3FReduceGradientsorVariable \\ --zip-file fileb://../ReduceGradients.zip CleanUp Create archive: cd ~/cloud-imaging/lambda/CleanUp zip -r9 ../CleanUp.zip . Create and upload Lambda function: # Upload function aws lambda create-function --function-name CleanUp \\ --zip-file fileb://../CleanUp.zip \\ --runtime python3.6 \\ --timeout 10 \\ --role copy_past_role_arn_here \\ --handler lambda_function.lambda_handler \\ --memory-size 128","title":"Upload Lambda functions"},{"location":"lambda/#lambda-functions","text":"All AWS Lambda functions used in the workflow are defined in the ~/cloud-imaging/lambda/ directory and each sub-directory contains the source code of the respective Lambda function. Some functions require the installation of additional python packages such as numpy . Follow the instructions to create deployment packages for the Lambda functions and to upload them to AWS. To create and upload the Lambda function, we need the Lambda service role that we created earlier. Log into the AWS console and go to Services -> IAM -> Roles -> SLIM-Extras_for_Lambda to find the ARN of this role.","title":"Lambda functions"},{"location":"lambda/#createqueues","text":"From within the ~/cloud-imaging/lambda/CreateQueues directory, run the following command to create a zip archive: cd ~/cloud-imaging/lambda/CreateQueues zip -r9 ../CreateQueue.zip . When uploading the function for the first time, run the following command to create a new Lambda function. Replace copy_paste_role_arn_here with the ARN of the SLIM-Extras_for_Lambda role (see above instructions of how to obtain the ARN): # Upload function aws lambda create-function --function-name CreateQueues \\ --zip-file fileb://../CreateQueues.zip \\ --runtime python3.6 \\ --timeout 300 \\ --role copy_paste_role_arn_here \\ --handler lambda_function.lambda_handler \\ --memory-size 128 If you make changes to the Lambda function and need to update an existing function, first create a new zip file with the above command and then run the following command: # Upload function aws lambda update-function-code --function-name IteratorStochastic \\ --zip-file fileb://../IteratorStochastic.zip","title":"CreateQueues"},{"location":"lambda/#iterator","text":"Create the zip archive: cd ~/cloud-imaging/lambda/Iterator zip -r9 ../Iterator.zip . Create and upload the Iterator Lambda function: # Upload function aws lambda create-function --function-name Iterator \\ --zip-file fileb://../Iterator.zip \\ --runtime python3.6 \\ --timeout 10 \\ --role copy_past_role_arn_here \\ --handler lambda_function.lambda_handler \\ --memory-size 128 Update an existing function with a new archive: # Upload function aws lambda update-function-code --function-name Iterator \\ --zip-file fileb://../Iterator.zip","title":"Iterator"},{"location":"lambda/#computegradients","text":"First, install numpy and boto3 inside the ComputeGradients directory and create a zip archive. (Lambda functions have boto3 pre-installed, but the default version is a deprecated version that misses some functionality, so we need to install the current release manually.) cd ~/cloud-imaging/lambda/ComputeGradients pip install --target . numpy boto3 zip -r9 ../ComputeGradients.zip . Then create and upload the ComputeGradients Lambda function: # Upload function aws lambda create-function --function-name ComputeGradients \\ --zip-file fileb://../ComputeGradients.zip \\ --runtime python3.6 \\ --timeout 900 \\ --role copy_past_role_arn_here \\ --handler lambda_function.gradient_handler \\ --memory-size 128 To update an existing function with a new archive, run: # Upload function aws lambda update-function-code --function-name ComputeGradients \\ --zip-file fileb://../ComputeGradients.zip For multi-node AWS Batch jobs, repeat the above steps for the ComputeGradients_MultiNode Lambda function. First create a zip archive called ComputeGradients_MultiNode.zip and then run: # Upload multi-node function aws lambda create-function --function-name ComputeGradients_MultiNode \\ --zip-file fileb://../ComputeGradients_MultiNode.zip \\ --runtime python3.6 \\ --timeout 900 \\ --role copy_past_role_arn_here \\ --handler lambda_function.gradient_handler \\ --memory-size 128","title":"ComputeGradients"},{"location":"lambda/#checks3forvariable","text":"Install python packages and create archive: cd ~/cloud-imaging/lambda/CheckS3ForVariable pip install --target . boto3 zip -r9 ../CheckS3ForVariable.zip . Create and upload Lambda function: # Upload function aws lambda create-function --function-name CheckS3ForVariable \\ --zip-file fileb://../CheckS3ForVariable.zip \\ --runtime python3.6 \\ --timeout 10 \\ --role copy_past_role_arn_here \\ --handler lambda_function.lambda_handler \\ --memory-size 128 Update the function: # Upload function aws lambda update-function-code --function-name CheckS3ForVariable \\ --zip-file fileb://../CheckS3ForVariable.zip For multi-node AWS Batch jobs, repeat the above steps for the CheckS3ForVariable_MultiNode Lambda function. Create a new zip file called CheckS3ForVariable_MultiNode.zip and then run: # Upload function aws lambda create-function --function-name CheckS3ForVariable_MultiNode \\ --zip-file fileb://../CheckS3ForVariable_MultiNode.zip \\ --runtime python3.6 \\ --timeout 10 \\ --role copy_past_role_arn_here \\ --handler lambda_function.lambda_handler \\ --memory-size 128","title":"CheckS3ForVariable"},{"location":"lambda/#reducegradients","text":"Install python packages and create archive: cd ~/cloud-imaging/lambda/ReduceGradients pip install --target . numpy boto3 zip -r9 ../ReduceGradients.zip . Create and upload Lambda function: # Upload function aws lambda create-function --function-name ReduceGradients \\ --zip-file fileb://../ReduceGradients.zip \\ --runtime python3.6 \\ --timeout 60 \\ --role copy_past_role_arn_here \\ --handler lambda_function.lambda_handler \\ --memory-size 512 Update the function: # Upload function aws lambda update-function-code --function-name CheckS3FReduceGradientsorVariable \\ --zip-file fileb://../ReduceGradients.zip","title":"ReduceGradients"},{"location":"lambda/#cleanup","text":"Create archive: cd ~/cloud-imaging/lambda/CleanUp zip -r9 ../CleanUp.zip . Create and upload Lambda function: # Upload function aws lambda create-function --function-name CleanUp \\ --zip-file fileb://../CleanUp.zip \\ --runtime python3.6 \\ --timeout 10 \\ --role copy_past_role_arn_here \\ --handler lambda_function.lambda_handler \\ --memory-size 128","title":"CleanUp"},{"location":"performance/","text":"Preparations before execution Reproducing the numerical examples and performance tests requires uploading the BP 2004 velocity model and data set to your S3 account. First, download the velocity model, the mask for the water bottom and the data set (7.4 GB) from the Georgia Tech FTP server: cd ~/cloud-imaging/numerical_examples wget ftp://slim.gatech.edu/data/users/pwitte/models/bp_synthetic_2004_velocity.h5 wget ftp://slim.gatech.edu/data/users/pwitte/models/bp_synthetic_2004_water_bottom.h5 wget ftp://slim.gatech.edu/data/users/pwitte/data/bp_synthetic_2004.tar.gz Extract the seismic data with tar -xzvf bp_synthetic_2004.tar.gz in the current directory. The models and the data need to be uploaded to an S3 bucket. Check if any pre-existing buckets are available in the AWS console -> Services -> S3 . If not, create a new bucket, such as slim-bucket-common (we will use this bucket name in the instructions, but you can choose a different name). We will upload the models to S3 with some meta data attached to it, to specify the grid spacing and origin. The script ~/cloud-imaging/numerical_examples/upload_files_to_s3.py automatically does this and uploads the models and data to S3. Before running this script, open it and fill in your S3 bucket name and the paths where you want to store the models. Follow this naming convention: paths for velocity and water model: your_user_name/models seismic data: your_user_name/data/ Update the script and run it from the directory into which you downloaded the data from the FTP server. Uploading the data will take a while, as there are 1,348 files to upload. Check whether the files have been uploaded successfully in the AWS console -> Services -> S3 -> bucket-name . Reproducing the examples Reproducing the numerical examples involves uploading the Python scripts of our application to S3. During runtime, the AWS Batch workers will fetch the script from S3 and run it to compute the gradients. Most examples do not involve running the full workflow, i.e. they do not require executing the AWS Step Function state machine. Instead, the launch_test.py scripts invoke the scaling tests for as single iteration of stochastic gradient descent (SGD), by manually triggering the Lambda function that is responsible for submitting the AWS Batch jobs. The launch_tests.py scripts also automatically collect the kernel and container run times from AWS and save them for plotting. Each example directory contains a job parameter file called parameters.json . Users are required to fill in the missing entries in each file, such as bucket names and the correct S3 paths. Weak Scaling The scripts for reproducing the weak scaling example are located in ~/cloud-imaging/numerical_examples/weak_scaling . 1) First, upload the Python script bp_synthetic_weak_scaling.py to S3. Use the AWS console in the browser to upload the file or use the command line interface: aws s3 cp bp_synthetic_weak_scaling.py s3://bucket-name/user-name/scripts/bp_synthetic_weak_scaling.py 2) The job parameters are specified in parameters.json . Fill in the following missing entries: bucket_name : name of your S3 bucket. partial_gradient_path : S3 path where partial gradients will be stored. The path name must follow the naming convention user_name/bp_partial_gradients/ . It is important not to omit the final / in the path name. full_gradient_path : S3 path name for full gradients. Naming convention: user_name/bp_full_gradients/ model_path : S3 paths in which models are stored. Naming convention: user_name/models/ variable_path : S3 paths in which images are stored. Naming convention: user_name/bp_variables/ script_path : S3 paths in which Python script is stored. Naming convention: user_name/scripts/ data_path : S3 path in which the seismic data is stored. Naming convention: user_name/data/ optional: container . Specifies the Docker image for AWS Batch. If you modified the Docker image and uploaded it to your AWS account, replace with the full name of the new Docker image (i.e. the image URI plus the tag). Do not modify the existing entries. 3) To compute the gradient of the BP model for a given batch size, run the launch_test.py script with the batch size and the run number as input arguments. The script invokes the ComputeGradients Lambda function to submit the AWS Batch job for the gradient computations. The script then waits until the job has finished, the gradient summation is completed and the updated image is written to the S3 bucket. The script collects all timings automatically from AWS and writes the results to a pickle file. Execute the following commands to run the example for a batch size of 2: export BATCHSIZE=2 export RUN_ID=0 python launch_test $BATCHSIZE $RUN_ID 4) To reproduce all weak scaling timings from the manuscript, run the shell script ./run_timings.sh , which executes the launch_test.py script for a batch size ranging from 1 to 128 . The script runs three consecutive timings per batch size and waits 700 seconds in between runs to ensure that AWS Batch has enough time to shut down all EC2 instances. Otherwise, AWS Batch uses already running instances in the consecutive timings and the time it takes AWS Batch to launch the EC2 instances is not accounted for. 5) Once all timings have been completed, the plot_weak_scaling_results.py script can be used to re-generate the figures from the manuscript. The original timings that are plotted in the manuscript are available in the results directory. Running the plotting script will create the figures using these timings. Stochastic gradient descent example To reproduce the full SGD example and run the Step Functions workflow for 30 iterations, follow the subsequent steps. All scripts for the examples are located in the ~/cloud-imaging/numerical_examples/imaging_example_sgd directory. 1) Upload the script bp_synthetic_sgd.py to Services -> S3 -> your-bucket -> user-name -> scripts (either using the AWS console or using the CLI as in the previous example). 2) All job parameters are specified in parameters.json . Fill in all missing entries, following the naming conventions as in the above \"Weak scaling\" example. Do not modify existing entries. 3) In this example, we execute the full Step Functions state machine, not just a Lambda function to compute a single SGD iteration. Start the Step Functions workflow by running the following command from within the ~/cloud-imaging/numerical_examples/imaging_example_sgd directory. First find the ARN of your state machine in the AWS console -> Step Functions -> LSRTM-SGD and then insert it into the following command: aws stepfunctions start-execution \\ --state-machine-arn arn:aws:states:us-east-1:xxxxxxxxxxxx:stateMachine:LSRTM-SGD \\ --input file://parameters.json 4) You can check the status of your workflow in the AWS console and get live updates of which task of the workflow is currently being executed. Go to the console -> Services -> Step Functions -> LSRTM-SGD and find the latest run in the Executions window 5) After the execution of the workflow has finished, re-create the seismic imaging from the manuscript with the script plot_final_image.py . Strong scaling with OpenMP Scripts for the OpenMP strong scaling examples are located in ~/cloud-imaging/numerical_examples/strong_scaling . 1) Upload the script bp_synthetic_omp_scaling_batch.py to Services -> S3 -> your-bucket -> user-name -> scripts 2) Fill in the missing entries of the job parameter file parameter.json . Follow the naming convention from before and do not modify existing entries. 3) To reproduce the timings with AWS Batch, either run the Python script launch_test.py for a single run or run the shell script run_timings_batch.sh to reproduce all timings (3 runs each). 4) To reproduce the timings on an EC2 bare metal instances, manually request a r5.metal instance from the AWS console -> Services -> EC2 -> Launch Instance . Ensure that ssh access to the instance is allowed by providing the corresponding security group. Once the instance is running, connect to the instance via ssh: ssh -i ~/.ssh/user_key_pair.pem -o StrictHostKeyChecking=no -l ubuntu public_DNS_of_instance On the instance, install git, clone the software repository and install all required packages: sudo apt-get update sudo apt-get install git-core git clone https://github.gatech.edu/pwitte3/aws_workflow cd aws_workflow/numerical_examples/strong_scaling ./setup.sh For the bare metal examples, there are no json parameter files. Instead, parameters are hard-coded into the main script bp_synthetic_omp_scaling_bare_metal.py . Before running the example, open the script and fill in the missing S3 paths that point to the models and seismic data. Run the timings using the shell script run_timings_bare_metal.sh : source ~/.bashrc ./run_timings_bare_metal.sh 5) All figures from the manuscript can be re-generated with the Python script plot_omp_timings.py . Strong scaling with MPI Scripts for the MPI strong scaling examples are located in ~/cloud-imaging/numerical_examples/strong_scaling_mpi . 1) Upload the scripts bp_synthetic_mpi_scaling.py and bp_synthetic_single.py to Services -> S3 -> your-bucket -> user-name -> scripts 2) Fill in the missing entries of the job parameter file parameter.json and parameter_single.json . The latter is the parameter file for running the application as on a single node. Follow the naming convention from before and do not modify existing entries. The parameter.json parameter file has one additional required field: user_id : Your AWS account number. This 12 digit number can be found in Services -> IAM or in the top-right corner of the AWS console. The parameter files are set up for running the timings on r5.24xlarge instances. To run the timings on c5n.18xlarge instances, change the following entries in the json parameter files: batch_queue : MultiNodeQueue_C5N_MAX_18 instance_type : c5n.18xlarge omp_num_threads : 18 3) To reproduce the strong scaling examples with MPI, either run the Python script launch_test.py for a single run or run the shell scripts run_timings.sh and run_timings_single.sh to reproduce all timings (3 runs each). 4) After reproducing the timings, generate the plots from the manuscript by running the script plot_mpi_strong_scaling.py and plot_mpi_strong_scaling_var.py . Hybrid example Scripts for the hybrid OpenMP-MPI examples are located in ~/cloud-imaging/numerical_examples/hybrid_scaling . 1) Upload the scripts bp_synthetic_hybrid.py and bp_synthetic_omp.py to S3 -> your-bucket -> user-name -> scripts 2) Fill in the missing entries of the job parameter file parameter.json . Follow the naming convention from before and do not modify existing entries. 3) To reproduce the timings, run the Python script launch_test.py for a single run or the shell script run_timings.sh to reproduce all timings (3 runs each). 4) After reproducing the timings, print the timings from the Table with the Python script print_kernel_times.py . To generate the plot, run the script plot_hybrid_timings.py . Cost examples Scripts for the cost comparison and cost saving strategies are located in ~/cloud-imaging/numerical_examples/cost . 1) Upload the script bp_synthetic_cost.py to S3 -> your-bucket -> user-name -> scripts 2) Fill in the missing entries of the job parameter file parameter.json . Follow the naming convention from before and do not modify existing entries. 3) To reproduce the timings for the cost comparisons, run the python script launch_test.py . The script computes a gradient of the BP model for a batch size of 100 and saves the runtimes as a pickle file. 4) To plot the cost comparison from the manuscript, run the Python script: plot_cost_comparison.py . 5) To plot the cost saving strategies for spot instances, use the following Python scripts: plot_cost_zone_c5n.py : compare zones for the c5n instance. plot_cost_zone_c1.py : compare zones for the c1 instance. plot_cost_type.py : compare different instances types within the us-east-1c zone. The scripts can automatically fetch the historic spot prices from AWS for a specified time period. However, historic spot prices are only available for a period of three months, so we provide For this reason, the historic spot prices shown in the manuscript are saved as pickle files. The above scripts. Resilience Scripts for the resilience example are located in ~/cloud-imaging/numerical_examples/resilience . 1) Upload the script bp_synthetic_cost.py to S3 -> your-bucket -> user-name -> scripts 2) Fill in the missing entries of the job parameter file parameter.json . Follow the naming convention from before and do not modify existing entries. 3) The script launch_test.py can be used to compute the gradient for the BP model for a given batch size and percentage of instance failures. To obtain the timings for a batch size of 100 and without failures (failure rate = 0), run: python launch_test.py 100 0 0`. where the first argument is batch size, the second argument is the run number and the third argument is the failure rate. To compute the gradient with 50 percent of the instances failing at random times during the run, type: python launch_test.py batch_size 0 0.5`. The timings from the manuscript for an increasing number of instance failures are modeled using the timings without instance failures and a two minute penalty for instance restarts. However, the script can be used to verify that the actual runtime with instance failures is predicted correctly by our model. 4) To re-create the figures from the manuscript and plot the results, run the Python script plot_resilience_modeled.py","title":"Performance tests"},{"location":"performance/#preparations-before-execution","text":"Reproducing the numerical examples and performance tests requires uploading the BP 2004 velocity model and data set to your S3 account. First, download the velocity model, the mask for the water bottom and the data set (7.4 GB) from the Georgia Tech FTP server: cd ~/cloud-imaging/numerical_examples wget ftp://slim.gatech.edu/data/users/pwitte/models/bp_synthetic_2004_velocity.h5 wget ftp://slim.gatech.edu/data/users/pwitte/models/bp_synthetic_2004_water_bottom.h5 wget ftp://slim.gatech.edu/data/users/pwitte/data/bp_synthetic_2004.tar.gz Extract the seismic data with tar -xzvf bp_synthetic_2004.tar.gz in the current directory. The models and the data need to be uploaded to an S3 bucket. Check if any pre-existing buckets are available in the AWS console -> Services -> S3 . If not, create a new bucket, such as slim-bucket-common (we will use this bucket name in the instructions, but you can choose a different name). We will upload the models to S3 with some meta data attached to it, to specify the grid spacing and origin. The script ~/cloud-imaging/numerical_examples/upload_files_to_s3.py automatically does this and uploads the models and data to S3. Before running this script, open it and fill in your S3 bucket name and the paths where you want to store the models. Follow this naming convention: paths for velocity and water model: your_user_name/models seismic data: your_user_name/data/ Update the script and run it from the directory into which you downloaded the data from the FTP server. Uploading the data will take a while, as there are 1,348 files to upload. Check whether the files have been uploaded successfully in the AWS console -> Services -> S3 -> bucket-name .","title":"Preparations before execution"},{"location":"performance/#reproducing-the-examples","text":"Reproducing the numerical examples involves uploading the Python scripts of our application to S3. During runtime, the AWS Batch workers will fetch the script from S3 and run it to compute the gradients. Most examples do not involve running the full workflow, i.e. they do not require executing the AWS Step Function state machine. Instead, the launch_test.py scripts invoke the scaling tests for as single iteration of stochastic gradient descent (SGD), by manually triggering the Lambda function that is responsible for submitting the AWS Batch jobs. The launch_tests.py scripts also automatically collect the kernel and container run times from AWS and save them for plotting. Each example directory contains a job parameter file called parameters.json . Users are required to fill in the missing entries in each file, such as bucket names and the correct S3 paths.","title":"Reproducing the examples"},{"location":"performance/#weak-scaling","text":"The scripts for reproducing the weak scaling example are located in ~/cloud-imaging/numerical_examples/weak_scaling . 1) First, upload the Python script bp_synthetic_weak_scaling.py to S3. Use the AWS console in the browser to upload the file or use the command line interface: aws s3 cp bp_synthetic_weak_scaling.py s3://bucket-name/user-name/scripts/bp_synthetic_weak_scaling.py 2) The job parameters are specified in parameters.json . Fill in the following missing entries: bucket_name : name of your S3 bucket. partial_gradient_path : S3 path where partial gradients will be stored. The path name must follow the naming convention user_name/bp_partial_gradients/ . It is important not to omit the final / in the path name. full_gradient_path : S3 path name for full gradients. Naming convention: user_name/bp_full_gradients/ model_path : S3 paths in which models are stored. Naming convention: user_name/models/ variable_path : S3 paths in which images are stored. Naming convention: user_name/bp_variables/ script_path : S3 paths in which Python script is stored. Naming convention: user_name/scripts/ data_path : S3 path in which the seismic data is stored. Naming convention: user_name/data/ optional: container . Specifies the Docker image for AWS Batch. If you modified the Docker image and uploaded it to your AWS account, replace with the full name of the new Docker image (i.e. the image URI plus the tag). Do not modify the existing entries. 3) To compute the gradient of the BP model for a given batch size, run the launch_test.py script with the batch size and the run number as input arguments. The script invokes the ComputeGradients Lambda function to submit the AWS Batch job for the gradient computations. The script then waits until the job has finished, the gradient summation is completed and the updated image is written to the S3 bucket. The script collects all timings automatically from AWS and writes the results to a pickle file. Execute the following commands to run the example for a batch size of 2: export BATCHSIZE=2 export RUN_ID=0 python launch_test $BATCHSIZE $RUN_ID 4) To reproduce all weak scaling timings from the manuscript, run the shell script ./run_timings.sh , which executes the launch_test.py script for a batch size ranging from 1 to 128 . The script runs three consecutive timings per batch size and waits 700 seconds in between runs to ensure that AWS Batch has enough time to shut down all EC2 instances. Otherwise, AWS Batch uses already running instances in the consecutive timings and the time it takes AWS Batch to launch the EC2 instances is not accounted for. 5) Once all timings have been completed, the plot_weak_scaling_results.py script can be used to re-generate the figures from the manuscript. The original timings that are plotted in the manuscript are available in the results directory. Running the plotting script will create the figures using these timings.","title":"Weak Scaling"},{"location":"performance/#stochastic-gradient-descent-example","text":"To reproduce the full SGD example and run the Step Functions workflow for 30 iterations, follow the subsequent steps. All scripts for the examples are located in the ~/cloud-imaging/numerical_examples/imaging_example_sgd directory. 1) Upload the script bp_synthetic_sgd.py to Services -> S3 -> your-bucket -> user-name -> scripts (either using the AWS console or using the CLI as in the previous example). 2) All job parameters are specified in parameters.json . Fill in all missing entries, following the naming conventions as in the above \"Weak scaling\" example. Do not modify existing entries. 3) In this example, we execute the full Step Functions state machine, not just a Lambda function to compute a single SGD iteration. Start the Step Functions workflow by running the following command from within the ~/cloud-imaging/numerical_examples/imaging_example_sgd directory. First find the ARN of your state machine in the AWS console -> Step Functions -> LSRTM-SGD and then insert it into the following command: aws stepfunctions start-execution \\ --state-machine-arn arn:aws:states:us-east-1:xxxxxxxxxxxx:stateMachine:LSRTM-SGD \\ --input file://parameters.json 4) You can check the status of your workflow in the AWS console and get live updates of which task of the workflow is currently being executed. Go to the console -> Services -> Step Functions -> LSRTM-SGD and find the latest run in the Executions window 5) After the execution of the workflow has finished, re-create the seismic imaging from the manuscript with the script plot_final_image.py .","title":"Stochastic gradient descent example"},{"location":"performance/#strong-scaling-with-openmp","text":"Scripts for the OpenMP strong scaling examples are located in ~/cloud-imaging/numerical_examples/strong_scaling . 1) Upload the script bp_synthetic_omp_scaling_batch.py to Services -> S3 -> your-bucket -> user-name -> scripts 2) Fill in the missing entries of the job parameter file parameter.json . Follow the naming convention from before and do not modify existing entries. 3) To reproduce the timings with AWS Batch, either run the Python script launch_test.py for a single run or run the shell script run_timings_batch.sh to reproduce all timings (3 runs each). 4) To reproduce the timings on an EC2 bare metal instances, manually request a r5.metal instance from the AWS console -> Services -> EC2 -> Launch Instance . Ensure that ssh access to the instance is allowed by providing the corresponding security group. Once the instance is running, connect to the instance via ssh: ssh -i ~/.ssh/user_key_pair.pem -o StrictHostKeyChecking=no -l ubuntu public_DNS_of_instance On the instance, install git, clone the software repository and install all required packages: sudo apt-get update sudo apt-get install git-core git clone https://github.gatech.edu/pwitte3/aws_workflow cd aws_workflow/numerical_examples/strong_scaling ./setup.sh For the bare metal examples, there are no json parameter files. Instead, parameters are hard-coded into the main script bp_synthetic_omp_scaling_bare_metal.py . Before running the example, open the script and fill in the missing S3 paths that point to the models and seismic data. Run the timings using the shell script run_timings_bare_metal.sh : source ~/.bashrc ./run_timings_bare_metal.sh 5) All figures from the manuscript can be re-generated with the Python script plot_omp_timings.py .","title":"Strong scaling with OpenMP"},{"location":"performance/#strong-scaling-with-mpi","text":"Scripts for the MPI strong scaling examples are located in ~/cloud-imaging/numerical_examples/strong_scaling_mpi . 1) Upload the scripts bp_synthetic_mpi_scaling.py and bp_synthetic_single.py to Services -> S3 -> your-bucket -> user-name -> scripts 2) Fill in the missing entries of the job parameter file parameter.json and parameter_single.json . The latter is the parameter file for running the application as on a single node. Follow the naming convention from before and do not modify existing entries. The parameter.json parameter file has one additional required field: user_id : Your AWS account number. This 12 digit number can be found in Services -> IAM or in the top-right corner of the AWS console. The parameter files are set up for running the timings on r5.24xlarge instances. To run the timings on c5n.18xlarge instances, change the following entries in the json parameter files: batch_queue : MultiNodeQueue_C5N_MAX_18 instance_type : c5n.18xlarge omp_num_threads : 18 3) To reproduce the strong scaling examples with MPI, either run the Python script launch_test.py for a single run or run the shell scripts run_timings.sh and run_timings_single.sh to reproduce all timings (3 runs each). 4) After reproducing the timings, generate the plots from the manuscript by running the script plot_mpi_strong_scaling.py and plot_mpi_strong_scaling_var.py .","title":"Strong scaling with MPI"},{"location":"performance/#hybrid-example","text":"Scripts for the hybrid OpenMP-MPI examples are located in ~/cloud-imaging/numerical_examples/hybrid_scaling . 1) Upload the scripts bp_synthetic_hybrid.py and bp_synthetic_omp.py to S3 -> your-bucket -> user-name -> scripts 2) Fill in the missing entries of the job parameter file parameter.json . Follow the naming convention from before and do not modify existing entries. 3) To reproduce the timings, run the Python script launch_test.py for a single run or the shell script run_timings.sh to reproduce all timings (3 runs each). 4) After reproducing the timings, print the timings from the Table with the Python script print_kernel_times.py . To generate the plot, run the script plot_hybrid_timings.py .","title":"Hybrid example"},{"location":"performance/#cost-examples","text":"Scripts for the cost comparison and cost saving strategies are located in ~/cloud-imaging/numerical_examples/cost . 1) Upload the script bp_synthetic_cost.py to S3 -> your-bucket -> user-name -> scripts 2) Fill in the missing entries of the job parameter file parameter.json . Follow the naming convention from before and do not modify existing entries. 3) To reproduce the timings for the cost comparisons, run the python script launch_test.py . The script computes a gradient of the BP model for a batch size of 100 and saves the runtimes as a pickle file. 4) To plot the cost comparison from the manuscript, run the Python script: plot_cost_comparison.py . 5) To plot the cost saving strategies for spot instances, use the following Python scripts: plot_cost_zone_c5n.py : compare zones for the c5n instance. plot_cost_zone_c1.py : compare zones for the c1 instance. plot_cost_type.py : compare different instances types within the us-east-1c zone. The scripts can automatically fetch the historic spot prices from AWS for a specified time period. However, historic spot prices are only available for a period of three months, so we provide For this reason, the historic spot prices shown in the manuscript are saved as pickle files. The above scripts.","title":"Cost examples"},{"location":"performance/#resilience","text":"Scripts for the resilience example are located in ~/cloud-imaging/numerical_examples/resilience . 1) Upload the script bp_synthetic_cost.py to S3 -> your-bucket -> user-name -> scripts 2) Fill in the missing entries of the job parameter file parameter.json . Follow the naming convention from before and do not modify existing entries. 3) The script launch_test.py can be used to compute the gradient for the BP model for a given batch size and percentage of instance failures. To obtain the timings for a batch size of 100 and without failures (failure rate = 0), run: python launch_test.py 100 0 0`. where the first argument is batch size, the second argument is the run number and the third argument is the failure rate. To compute the gradient with 50 percent of the instances failing at random times during the run, type: python launch_test.py batch_size 0 0.5`. The timings from the manuscript for an increasing number of instance failures are modeled using the timings without instance failures and a two minute penalty for instance restarts. However, the script can be used to verify that the actual runtime with instance failures is predicted correctly by our model. 4) To re-create the figures from the manuscript and plot the results, run the Python script plot_resilience_modeled.py","title":"Resilience"},{"location":"roles/","text":"IAM roles AWS manages access to its services through roles . Users and AWS services such as Lambda functions require explicit permissions to interact with other services or to request computational resources. User roles provide permissions to a specific IAM user, while service roles allow specific AWS services to interact with each other. For example, to start an AWS Batch job from the command line, users require the AWSBatchFullAccess user role. If we want to allow a container launched by AWS Batch to send messages to an SQS queue, we need to provide an AWS service role for AWS Batch and attach the AmazonSQSFullAccess policy to it. The following instructions create the necessary user and service roles for our workflow. User roles Log into the AWS console ( https://console.aws.amazon.com/console ) and check if the following roles are attached to your user in Services -> IAM -> Users -> your_user_name . Run the following commands in a terminal to obtain the missing permissions that are not attached to your account so far. (Replace your_user_name by your IAM user name). # EC2 aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AmazonEC2FullAccess # Batch aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AWSBatchFullAccess # ECR aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess # Lambda aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AWSLambdaFullAccess # SQS aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AmazonSQSFullAccess # Step Functions aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AWSStepFunctionsFullAccess # S3 Read aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Service roles The following service roles are required for the workflow: AWSBatchServiceRole Check if the role exists in your AWS console under IAM -> Roles . If not, open a terminal in the current directory ( ~/aws_workflow ) and run the following commands: # Create role aws iam create-role --role-name AWSBatchServiceRole \\ --assume-role-policy-document file://service_roles/create_AWSBatchServiceRole.json \\ --description \"Allows Batch to create and manage AWS resources on your behalf.\" # Attach policy aws iam attach-role-policy --role-name AWSBatchServiceRole --policy-arn \\ arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole StepFunctionsLambdaRole Check if the role exists in IAM -> Roles . If not, open a terminal in the current directory ( ~/cloud-imaging ) and run the following commands: # Create role aws iam create-role --role-name StepFunctionsLambdaRole \\ --assume-role-policy-document file://service_roles/create_StepFunctionsLambdaRole.json \\ --description \"Allows Step Functions to access AWS resources on your behalf.\" # Attach policy aws iam attach-role-policy --role-name StepFunctionsLambdaRole --policy-arn \\ arn:aws:iam::aws:policy/service-role/AWSLambdaRole SLIM-Extras_ECS_for_EC2 Create this role, regardless of whether any other ECS roles exist so far: # Create role aws iam create-role --role-name SLIM-Extras_ECS_for_EC2 \\ --assume-role-policy-document file://service_roles/create_SLIM-Extras_ECS_for_EC2.json \\ --description \"Allows EC2 instances in an ECS cluster to access ECS.\" # Attach policies aws iam attach-role-policy --role-name SLIM-Extras_ECS_for_EC2 --policy-arn \\ arn:aws:iam::aws:policy/AmazonSQSFullAccess aws iam attach-role-policy --role-name SLIM-Extras_ECS_for_EC2 --policy-arn \\ arn:aws:iam::aws:policy/AmazonS3FullAccess aws iam attach-role-policy --role-name SLIM-Extras_ECS_for_EC2 --policy-arn \\ arn:aws:iam::aws:policy/IAMReadOnlyAccess aws iam attach-role-policy --role-name SLIM-Extras_ECS_for_EC2 --policy-arn \\ arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role SLIM-Extras_for_Lambda Create this role, regardless of whether any other Lambda roles exist so far: # Create role aws iam create-role --role-name SLIM-Extras_for_Lambda \\ --assume-role-policy-document file://service_roles/create_SLIM-Extras_for_Lambda.json \\ --description \"Allows Lambda functions to call AWS services on your behalf.\" # Attach policies aws iam attach-role-policy --role-name SLIM-Extras_for_Lambda --policy-arn \\ arn:aws:iam::aws:policy/AmazonSQSFullAccess aws iam attach-role-policy --role-name SLIM-Extras_for_Lambda --policy-arn \\ arn:aws:iam::aws:policy/AmazonS3FullAccess aws iam attach-role-policy --role-name SLIM-Extras_for_Lambda --policy-arn \\ arn:aws:iam::aws:policy/AWSLambdaFullAccess aws iam attach-role-policy --role-name SLIM-Extras_for_Lambda --policy-arn \\ arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole # Create and attach specialized Lambda for Batch policy aws iam create-policy --policy-name LambdaBatchExecutionPolicy \\ --policy-document file://service_roles/create_LambdaBatchExecutionPolicy.json\\ --description \"Allow Lambda to access AWS Batch services including job registration and submission.\" aws iam attach-role-policy --role-name SLIM-Extras_for_Lambda \\ --policy-arn arn:aws:iam::851065145468:policy/LambdaBatchExecutionPolicy Roles for using Spot instances with Batch Create the following roles to enable spot instances for usage with Batch: # AmazonEC2SpotFleetRole aws iam create-role --role-name AmazonEC2SpotFleetRole --assume-role-policy-document '{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"spotfleet.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}' # AWSServiceRoleForEC2Spot aws iam create-service-linked-role --aws-service-name spot.amazonaws.com # AWSServiceRoleForEC2SpotFleet aws iam create-service-linked-role --aws-service-name spotfleet.amazonaws.com","title":"Create IAM roles"},{"location":"roles/#iam-roles","text":"AWS manages access to its services through roles . Users and AWS services such as Lambda functions require explicit permissions to interact with other services or to request computational resources. User roles provide permissions to a specific IAM user, while service roles allow specific AWS services to interact with each other. For example, to start an AWS Batch job from the command line, users require the AWSBatchFullAccess user role. If we want to allow a container launched by AWS Batch to send messages to an SQS queue, we need to provide an AWS service role for AWS Batch and attach the AmazonSQSFullAccess policy to it. The following instructions create the necessary user and service roles for our workflow.","title":"IAM roles"},{"location":"roles/#user-roles","text":"Log into the AWS console ( https://console.aws.amazon.com/console ) and check if the following roles are attached to your user in Services -> IAM -> Users -> your_user_name . Run the following commands in a terminal to obtain the missing permissions that are not attached to your account so far. (Replace your_user_name by your IAM user name). # EC2 aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AmazonEC2FullAccess # Batch aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AWSBatchFullAccess # ECR aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess # Lambda aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AWSLambdaFullAccess # SQS aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AmazonSQSFullAccess # Step Functions aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AWSStepFunctionsFullAccess # S3 Read aws iam attach-user-policy --user-name your_user_name --policy-arn \\ arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess","title":"User roles"},{"location":"roles/#service-roles","text":"The following service roles are required for the workflow: AWSBatchServiceRole Check if the role exists in your AWS console under IAM -> Roles . If not, open a terminal in the current directory ( ~/aws_workflow ) and run the following commands: # Create role aws iam create-role --role-name AWSBatchServiceRole \\ --assume-role-policy-document file://service_roles/create_AWSBatchServiceRole.json \\ --description \"Allows Batch to create and manage AWS resources on your behalf.\" # Attach policy aws iam attach-role-policy --role-name AWSBatchServiceRole --policy-arn \\ arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole StepFunctionsLambdaRole Check if the role exists in IAM -> Roles . If not, open a terminal in the current directory ( ~/cloud-imaging ) and run the following commands: # Create role aws iam create-role --role-name StepFunctionsLambdaRole \\ --assume-role-policy-document file://service_roles/create_StepFunctionsLambdaRole.json \\ --description \"Allows Step Functions to access AWS resources on your behalf.\" # Attach policy aws iam attach-role-policy --role-name StepFunctionsLambdaRole --policy-arn \\ arn:aws:iam::aws:policy/service-role/AWSLambdaRole SLIM-Extras_ECS_for_EC2 Create this role, regardless of whether any other ECS roles exist so far: # Create role aws iam create-role --role-name SLIM-Extras_ECS_for_EC2 \\ --assume-role-policy-document file://service_roles/create_SLIM-Extras_ECS_for_EC2.json \\ --description \"Allows EC2 instances in an ECS cluster to access ECS.\" # Attach policies aws iam attach-role-policy --role-name SLIM-Extras_ECS_for_EC2 --policy-arn \\ arn:aws:iam::aws:policy/AmazonSQSFullAccess aws iam attach-role-policy --role-name SLIM-Extras_ECS_for_EC2 --policy-arn \\ arn:aws:iam::aws:policy/AmazonS3FullAccess aws iam attach-role-policy --role-name SLIM-Extras_ECS_for_EC2 --policy-arn \\ arn:aws:iam::aws:policy/IAMReadOnlyAccess aws iam attach-role-policy --role-name SLIM-Extras_ECS_for_EC2 --policy-arn \\ arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role SLIM-Extras_for_Lambda Create this role, regardless of whether any other Lambda roles exist so far: # Create role aws iam create-role --role-name SLIM-Extras_for_Lambda \\ --assume-role-policy-document file://service_roles/create_SLIM-Extras_for_Lambda.json \\ --description \"Allows Lambda functions to call AWS services on your behalf.\" # Attach policies aws iam attach-role-policy --role-name SLIM-Extras_for_Lambda --policy-arn \\ arn:aws:iam::aws:policy/AmazonSQSFullAccess aws iam attach-role-policy --role-name SLIM-Extras_for_Lambda --policy-arn \\ arn:aws:iam::aws:policy/AmazonS3FullAccess aws iam attach-role-policy --role-name SLIM-Extras_for_Lambda --policy-arn \\ arn:aws:iam::aws:policy/AWSLambdaFullAccess aws iam attach-role-policy --role-name SLIM-Extras_for_Lambda --policy-arn \\ arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole # Create and attach specialized Lambda for Batch policy aws iam create-policy --policy-name LambdaBatchExecutionPolicy \\ --policy-document file://service_roles/create_LambdaBatchExecutionPolicy.json\\ --description \"Allow Lambda to access AWS Batch services including job registration and submission.\" aws iam attach-role-policy --role-name SLIM-Extras_for_Lambda \\ --policy-arn arn:aws:iam::851065145468:policy/LambdaBatchExecutionPolicy Roles for using Spot instances with Batch Create the following roles to enable spot instances for usage with Batch: # AmazonEC2SpotFleetRole aws iam create-role --role-name AmazonEC2SpotFleetRole --assume-role-policy-document '{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"spotfleet.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}' # AWSServiceRoleForEC2Spot aws iam create-service-linked-role --aws-service-name spot.amazonaws.com # AWSServiceRoleForEC2SpotFleet aws iam create-service-linked-role --aws-service-name spotfleet.amazonaws.com","title":"Service roles"},{"location":"sqs/","text":"SQS queues Create an SQS queue for the gradient summation using the following command: aws sqs create-queue --queue-name GradientQueue_1 \\ --attributes DelaySeconds=0,ReceiveMessageWaitTimeSeconds=0,VisibilityTimeout=60 Once we have defined the queue, we have to tell the queue which Lambda function to invoke when it is triggered. First, find the ARN of the queue that we just created. Go to the console -> Simple Queue Service -> GradientQueue_1 , copy the ARN and use it in the following command: aws lambda create-event-source-mapping \\ --event-source-arn arn_of_sqs_queue \\ --function-name ReduceGradients You can verify that this trigger was created correctly by clicking on the queue and selecting the Lambda Triggers tab at the bottom of the page. The ReduceGradients function should be listed as a trigger. If we run our full worklow by executing the AWS Step Functions state machine, we can specify that we want the SQS queue(s) to be be set up automatically. In this case, set the auto_create_queues parameter of the ~/cloud-imaging/numerical_examples/imaging_example_sgd/parameters.json file to TRUE . After executing the workflow, the queue is automatically removed.","title":"Create message queues"},{"location":"sqs/#sqs-queues","text":"Create an SQS queue for the gradient summation using the following command: aws sqs create-queue --queue-name GradientQueue_1 \\ --attributes DelaySeconds=0,ReceiveMessageWaitTimeSeconds=0,VisibilityTimeout=60 Once we have defined the queue, we have to tell the queue which Lambda function to invoke when it is triggered. First, find the ARN of the queue that we just created. Go to the console -> Simple Queue Service -> GradientQueue_1 , copy the ARN and use it in the following command: aws lambda create-event-source-mapping \\ --event-source-arn arn_of_sqs_queue \\ --function-name ReduceGradients You can verify that this trigger was created correctly by clicking on the queue and selecting the Lambda Triggers tab at the bottom of the page. The ReduceGradients function should be listed as a trigger. If we run our full worklow by executing the AWS Step Functions state machine, we can specify that we want the SQS queue(s) to be be set up automatically. In this case, set the auto_create_queues parameter of the ~/cloud-imaging/numerical_examples/imaging_example_sgd/parameters.json file to TRUE . After executing the workflow, the queue is automatically removed.","title":"SQS queues"},{"location":"step/","text":"State machine for single-node jobs The AWS Step Functions workflow is defined as a json file in ~/cloud-imaging/step_functions/state_machine.json . The file does not need to be modified. To create the workflow for the first time, run the following command from a terminal within the ~/cloud-imaging directory. This step requires the ARN of the Step Functions service role. The ARN can be found in the AWS console -> Services -> IAM -> Roles -> StepFunctionsLambdaRole . Copy the ARN and use it in the following command: aws stepfunctions create-state-machine --name LSRTM-SGD \\ --role-arn step_functions_service_role_arn \\ --definition file://step_functions/state_machine.json To upload an updated version of the workflow, first find the ARN of the state machine that we created with the above command. Go to the AWS console -> Step Functions -> LSRTM-SGD and copy the ARN. Then run the following command using this ARN: aws stepfunctions update-state-machine \\ --state-machine-arn state_machine_arn \\ --definition file://step_functions/state_machine.json State machine for multi-node jobs To set up the state-machine for the multi-node workflow, follow the same steps as above. The multi-node workflow is defined in ~/cloud-imaging/step_functions/state_machine_multinode.json . Obtain the ARN of the StepFunctionsLambdaRole (as described above) and upload the workflow as follows: aws stepfunctions create-state-machine --name LSRTM-SGD-MultiNode \\ --role-arn step_functions_service_role_arn \\ --definition file://step_functions/state_machine_multinode.json","title":"Step Functions"},{"location":"step/#state-machine-for-single-node-jobs","text":"The AWS Step Functions workflow is defined as a json file in ~/cloud-imaging/step_functions/state_machine.json . The file does not need to be modified. To create the workflow for the first time, run the following command from a terminal within the ~/cloud-imaging directory. This step requires the ARN of the Step Functions service role. The ARN can be found in the AWS console -> Services -> IAM -> Roles -> StepFunctionsLambdaRole . Copy the ARN and use it in the following command: aws stepfunctions create-state-machine --name LSRTM-SGD \\ --role-arn step_functions_service_role_arn \\ --definition file://step_functions/state_machine.json To upload an updated version of the workflow, first find the ARN of the state machine that we created with the above command. Go to the AWS console -> Step Functions -> LSRTM-SGD and copy the ARN. Then run the following command using this ARN: aws stepfunctions update-state-machine \\ --state-machine-arn state_machine_arn \\ --definition file://step_functions/state_machine.json","title":"State machine for single-node jobs"},{"location":"step/#state-machine-for-multi-node-jobs","text":"To set up the state-machine for the multi-node workflow, follow the same steps as above. The multi-node workflow is defined in ~/cloud-imaging/step_functions/state_machine_multinode.json . Obtain the ARN of the StepFunctionsLambdaRole (as described above) and upload the workflow as follows: aws stepfunctions create-state-machine --name LSRTM-SGD-MultiNode \\ --role-arn step_functions_service_role_arn \\ --definition file://step_functions/state_machine_multinode.json","title":"State machine for multi-node jobs"},{"location":"trouble/","text":"Troubleshooting Setting up the workflow is unfortunately a little tricky, as a large number of components have to be set up manually. The best reference point for questions and troubleshooting is the AWS documentation of the respective services: AWS EC2: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html AWS Batch: https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html AWS multi-node jobs: https://docs.aws.amazon.com/batch/latest/userguide/multi-node-parallel-jobs.html AWS Lambda: https://docs.aws.amazon.com/lambda/latest/dg/welcome.html AWS S3: https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html AWS Step Functions: https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html AWS SQS: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html For questions, please contact Philipp Witte at pwitte3@gatech.edu .","title":"Troubleshooting"},{"location":"trouble/#troubleshooting","text":"Setting up the workflow is unfortunately a little tricky, as a large number of components have to be set up manually. The best reference point for questions and troubleshooting is the AWS documentation of the respective services: AWS EC2: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html AWS Batch: https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html AWS multi-node jobs: https://docs.aws.amazon.com/batch/latest/userguide/multi-node-parallel-jobs.html AWS Lambda: https://docs.aws.amazon.com/lambda/latest/dg/welcome.html AWS S3: https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html AWS Step Functions: https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html AWS SQS: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html For questions, please contact Philipp Witte at pwitte3@gatech.edu .","title":"Troubleshooting"}]}